# Vit_action: Human Activity Recognition using Vision Transformers

This repository implements a human activity recognition model leveraging Vision Transformers (ViTs). The model is designed to recognize and classify human activities based on the Stanford Action Dataset, which contains a diverse set of labeled action categories.

The Vision Transformer (ViT) architecture, known for its effectiveness in capturing spatial relationships in visual data, is utilized to analyze sequences of images and extract high-level features relevant to specific actions. By training the ViT model on the Stanford Action Dataset, the system can accurately identify and categorize various human activities, including both simple and complex actions.

Key features include:
- Implementation of the Vision Transformer architecture for video frame-based action recognition.
- Preprocessing and augmentation techniques tailored to the Stanford Action Dataset.
- Training scripts to fine-tune the ViT model for improved accuracy.
- Evaluation metrics and performance tracking to benchmark model results against baseline methods.
- Detailed documentation and instructions for setup, training, and evaluation.

This repository serves as a resource for researchers and developers interested in applying state-of-the-art transformer-based architectures to human activity recognition tasks.
